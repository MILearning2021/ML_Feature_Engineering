{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XFItDM5XCZmj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd # Для работы с данными\n",
    "import matplotlib.pyplot as plt  # Библиотека для визуализации результатов \n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPqDB-HECZm0"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://netology.ru/backend/tilda/images/tild3439-3364-4535-b334-656263633534__main.svg\"  width=900></p>\n",
    "<h3 style=\"text-align: center;\"><b>Метрики близости. Работа с текстовыми данными. Наивный Байес.</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsUBVpS_CZm2"
   },
   "source": [
    "<h1><u>План урока</u></h1>\n",
    "\n",
    "<p><font size=\"3\" face=\"Arial\">\n",
    "<ul type=\"square\"><a href=\"#1\"><li>Основные задачи.</li></a><a href=\"#2\"><li>Методы работы.</li></a><ul><a href=\"#3\"><li>TF IDF.</li></a><a href=\"#4\"><li>Векторное представление слов.</li></a><a href=\"#5\"><li>word2vec.</li></a></ul><a href=\"#6\"><li>Мера близости.</li></a><a href=\"#7\"><li>Наивный Байес.</li></a><a href=\"#8\"><li>Практика.</li></a>\n",
    "    <ul><a href=\"#8\"><li>Методы на практике.</li></a><a href=\"#9\"><li>Распознавание спама.</li></a></ul>    \n",
    "</ul></font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVVXoQRKCZm3"
   },
   "source": [
    "<h2>Основные задачи</h2>\n",
    "<p id=\"1\">Работа с текстовыми данными ведется во многих областях бизнеса и исследований. C каждым годом появляются все более эффективные методы, дающие возможность улучшить обучение моделей на основе текстовых данных. Ситуаций и задач, при которых мы можем столкнуться с работой с текстом может быть огромное кол-во, вот лишь некоторые из них:</p>\n",
    "<div class=\"alert alert-info\"><ul><li>Машинный перевод</li><li>Классификация текстов</li><ul><li>Фильтрация спама</li><li>По тональности, теме или жанру</li></ul><li>Кластеризация текстов</li><li>Извлечение информации</li><ul><li>Фактов и событий</li><li>Именованных сущностей</li></ul><li>Вопросно-ответные системы</li><li>Генерация текстов</li><li>Распознавание речи</li><li>Проверка правописания</li></ul></div>\n",
    "<p style=\"align: center;\"><img align=center src=\"https://automated-testing.info/uploads/default/original/2X/1/1f355c92a192e7b4b98e3bec60ea5c6cd72d999e.jpeg\"  width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EikcQGnOCZm4"
   },
   "source": [
    "<h2>Методы работы</h2>\n",
    "<p id=\"2\">Итак, перед нами стоит задача работы с текстовыми данными. Компьютер понимает только числа, отсюда следует, что каким-то образом слова нужно кодировать для нормальной работы с ними. Рассмотрим некоторые методы работы с ними.</p>\n",
    "<h3>TF IDF</h3>\n",
    "<p id=\"3\">Одним из самых простых и древнейших методов работы с текстом является коэффициент TF IDF, который состоит из произведения двух чисел. TF IDF - это некая статистическая мера, используемая для оценки важности слова в контексте документа.</p>\n",
    "<ul><li><b>TF</b> (term frequency - частота слова) - отношение числа вхождений некоторого слова к общему числу слов документа. Таким, образом, оценивается важность слова в пределах отдельного документа.</li><li><b>IDF</b> (inverse document frequency - обратная частота документа) - инверсия частоты, с которой некоторое слово встречается в документах коллекции. Для каждого уникального слова в пределах конкретной коллекции документов существует только единственное значение IDF.</li></ul>\n",
    "<p style=\"align: center;\"><img align=center src=\"https://www.researchgate.net/profile/Haider-Al-Khateeb/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png\"  width=900></p>\n",
    "<p>Итого имеем - мера TF IDF является произведением сомножителей:</p>\n",
    "$$\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D) ,$$\n",
    "$$\\text{где    } \\text{    tf}(t, d) = \\frac{n_t}{\\sum_k n_k}  ;   \\text{    idf}(t, D) = \\ln \\left(\\frac{|D|}{|{d \\in D: t \\in d}|} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhr20cguCZm6"
   },
   "source": [
    "<h3>Векторное представление слов</h3>\n",
    "<p id=\"4\">Вскоре был придуман совсем новый подход, в отличие от традиционных методов - векторное представление. Каждое слово представляется как точка в пространстве с фиксированной размерностью. Все работает без разметки данных (unsupervised), в отличие от традиционных методов.К примеру слово \"Hello\" может быть представлено как [0.4, -0.11, 0.55, 0.3, ... , 0.02].</p>\n",
    "\n",
    "<p style=\"align: center;\"><img align=center src=\"https://miro.medium.com/max/2400/1*2r1yj0zPAuaSGZeQfG6Wtw.png\"  width=900></p>\n",
    "<p><i>Но как решать эту задачу? Как строить и обучать модель?</i> Один из способов - word2vec.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KSWxdczCZm7"
   },
   "source": [
    "<h3>word2vec</h3>\n",
    "<p id=\"5\">В методе word2vec решается 2 типа задач. Первый ти - это когда по контексту (несколько слов до, и несколько слов после слова) предсказывается слово. Второй - по слову предсказывается контекст. Таким, образом решая эту задачу мы приходим к тому, что близкие по контексту слова имеют близкое представление в векторном представлении. И отсюда имеем, что синонимы в векторном представлении слов находятся рядом.</p>\n",
    "<p style=\"align: center;\"><img align=center src=\"https://sun9-17.userapi.com/impg/IderJ222cIEsqhyI9vPOJeWu4JiY99sg7wWQhg/Ym-qzNGwLkY.jpg?size=981x605&quality=96&sign=0c9c9faf22464e75c67c1063f140242a&type=album\"  width=900></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AODtysWGCZm8"
   },
   "source": [
    "<h2>Мера близости</h2>\n",
    "<p id=\"6\">После того как мы успешно закодировали тем или иным способом и получали информацию о словах в предложении, зачастую встает задача о сравнении предложений, извлечении смысла, и т.д. Тут необходимо ввести такое понятие как мера близости. Ниже представлены три основныз меры для веторных форм.</p>\n",
    "<div class=\"alert alert-info\"><ul><li>Эвклидова норма $||x||_e = \\sqrt{\\sum\\limits_{i=1}^n |x_i|^2}$, и расстояние $\\rho_2(x,y) = ||x-y||_2 =\\sqrt{\\sum\\limits_{i=1}^n (x_i-y_i)^2} $</li>\n",
    "<li>Манхэттенская норма $||x||_m = \\sum\\limits_{i=1}^n |x_i|$, и расстояние $\\rho_1(x,y) = ||x-y||_1 =\\sum\\limits_{i=1}^n |x_i-y_i| $ </li><li>Косинусное расстояния $ \\left( x,y \\right) = |x|\\cdot |y|cos(\\alpha) \\rightarrow cos(\\alpha)=\\frac{(x,y)}{|x|\\cdot |y|} $</li></ul></div>\n",
    "<p>Также необходимо ввести меру семантической близости между словами, до кодирования вектором.<br>\n",
    "<b>Мера семантической близости</b> - это особая мера близости, предназначенная для количественной оценки семантической схожести лексем, например, существительных или многословных выражений.\n",
    "<ul><li>Расстояние Левенштейна - минимальное количество односимвольных операций (вставки, удаления, замены).</li><li>Расстояние Хэмминга - число позиций, в которых соответствующие символы двух слов одинаковой длины различны.</li><li>Коэффициент Жаккара - это мера, основанная на использовании информации о множестве общих симоволов и равна отношению пересечения двух множеств к их объединению. $$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$$</li></ul>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psupjbDKCZm-"
   },
   "source": [
    "<h2>Наивный Байес</h2>\n",
    "<p id=\"7\">Во многих проектах/задачах нужно тем или иным способом по тексту определить тональность: положительная или отрицательная. Один из методов, который может помочь в этом - наивный Байес. <br>\n",
    "<b>Наивный байесовский классификатор </b>- простой вероятностный классификатор, основанный на применении теоремы Байеса со строгими (наивными) предположениями о независимости. Т.е. считается, что каждое слово появляется абсолютно независимо от других. Достоинством наивного байесовского классификатора является малое кол-во данных, необходимых для обучения, оценки параметров и классификации. </p>\n",
    "$$\\hat{q} = arg\\max [P(Q_k)\\prod\\limits_{i =  1}^n P(x_i | Q_k)]$$\n",
    "где $P(Q_k)$ - отношения числа документов класса $Q_k$ к общему числу, а $P(x_i | Q_k) = \\frac{\\alpha + N_{ik}}{\\alpha M + N_k}$ - вхождение слова $x_i$ в документ класса $Q_k$.<br>\n",
    "<p style=\"align: center;\"><img align=center src=\"https://sun9-35.userapi.com/impg/30rfGDrV-FaWdiNALoY4Zo77hZaVmiCoQfzb-A/EBEERUkpOr4.jpg?size=806x652&quality=96&sign=ddad78b9e762422328d806da656974df&type=album\"  width=900></p>\n",
    "<p>Подробно и с мат. док-вами, доп. информацию, можно почитать <a href=\"https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9_%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80\">здесь</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuUZFyfuCZm_"
   },
   "source": [
    "<h2>Практика</h2>\n",
    "<h3>Методы на практике</h3>\n",
    "<p id=\"8\">Для начала посмотрим методы на практике, рассмотренные сегодня выше, а затем будем решать задачу определения спама.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrXYJbmgCZnA",
    "outputId": "99823f1f-096a-46ff-8658-d7825e34237f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Library/Frameworks/Python.framework/Versions/3.9/include/python3.9/UNKNOWN\n",
      "sysconfig: /Library/Frameworks/Python.framework/Versions/3.9/include/python3.9\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for docopt, since package 'wheel' is not installed.\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "    Running setup.py install for docopt ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Library/Frameworks/Python.framework/Versions/3.9/include/python3.9/UNKNOWN\n",
      "sysconfig: /Library/Frameworks/Python.framework/Versions/3.9/include/python3.9\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[?25hSuccessfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "\u001b[33mWARNING: You are using pip version 21.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2     \n",
    "# устанавливаем необходимую библиотеку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DppEvaXDCZnB"
   },
   "source": [
    "Рассмотрим сначала метод кодирования, делающий из слов one-hot вектора. Данный метод CountVectorizer для каждого предложения возвращает вектор, который несет в себе информацию, какие слова есть в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Gg7XdjZsCZnC"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3csJJkCCZnD"
   },
   "source": [
    "Получаем вектора для каждого предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlYBm3YpCZnE",
    "outputId": "2f5668ec-10d1-46f8-fb30-2f745846fc35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYXNbPUOCZnG"
   },
   "source": [
    "Также можно учитывать не только отдельно стоящие слова, а словосочетания. Параметр ngram_range задает кол-во слов в словосочетании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gjh9Zc7-CZnH",
    "outputId": "f6a77aab-1b1f-4cac-feaf-bca334923efb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and this', 'and this is', 'document', 'document is', 'document is the', 'first', 'first document', 'is', 'is the', 'is the first', 'is the second', 'is the third', 'is this', 'is this the', 'one', 'second', 'second document', 'the', 'the first', 'the first document', 'the second', 'the second document', 'the third', 'the third one', 'third', 'third one', 'this', 'this document', 'this document is', 'this is', 'this is the', 'this the', 'this the first']\n",
      "[[0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0]\n",
      " [0 0 0 2 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5AORdebCZnI"
   },
   "source": [
    "Теперь посчитаем коэффициенты TF IDF. В данном случае указана (номер документа, позиция слова в словаре) и соответствующий TF IDF коэффициент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d34dt9emCZnJ",
    "outputId": "f0659880-8414-4a09-f2f8-e317a637bc7a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.46979138557992045\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 8)\t0.38408524091481483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwILn3DuYAyr"
   },
   "source": [
    "Теперь посмотрим библиотеки и методы работы для разбивки на слова/предложения и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMYIjl5OCZnJ",
    "outputId": "991c4314-9c5f-4130-dc59-0b263aea3ba5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ivanpetrov/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ivanpetrov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # полезная библиотека решающая вопросы с пунктуацией, словами и т.д.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YeOIXc_CZnK"
   },
   "source": [
    "Протокенизируем предложения (=разделить пословно и создать массив токенов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5VmSHZv3CZnL"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF_SrCW3CZnM"
   },
   "source": [
    "Видим, что у нас был text = 3 предложение. tokenize поделил наш текст на три предложения-массива, и затем каждое предложение поделил на слова-токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3-4mAY9CZnM",
    "outputId": "2266ab5e-9786-4c96-dca0-04dd04a37537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qklOn3nZCZnN"
   },
   "source": [
    "С английским все хорошо работает. Теперь попробуем сделать это с русским. Разобьем исходный текст на предложения, из каждого предложения, с помощью регулярных выражений, удалим все символы не являющиеся буквами (знаки препинания, пробелы) и разделим на слова-токены. Затем удалим стоп-слова (союзы, междометия, предлоги и т.д.). И наконец, с помощью библиотеки pymorphy мы можем получить инфинитив (нач. форму) каждого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzUDUXeECZnN",
    "outputId": "23459c99-c07f-4a6b-a0c2-b64c94ee3e4d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymorphy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1a08e00c3d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# подключаем стопслова\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy2'"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = stopwords.words('russian') # подключаем стопслова\n",
    "\n",
    "text = \"Обработка текстов на естественном языке — общее направление искусственного интеллекта и математической лингвистики. Оно изучает проблемы компьютерного анализа и синтеза текстов на естественных языках. Применительно к искусственному интеллекту анализ означает понимание языка, а синтез — генерацию грамотного текста.\"\n",
    "sentences = nltk.sent_tokenize(text, language=\"russian\") # разбиваем на предложения\n",
    "for sentence in sentences:\n",
    "    print('___________________')\n",
    "    print(sentence)\n",
    "    sentence_ = re.sub(r\"[^А-Яа-яёЁ ]\",\"\", sentence)\n",
    "    tokens = nltk.word_tokenize(sentence_) # разбиваем на слова\n",
    "    tokens = [i for i in tokens if (i not in stop_words)] # исключаем стопслова\n",
    "    tokens = list(map(lambda x: morph.parse(x)[0].normal_form, tokens)) # приводим к нормальной форме (parse)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRr4vdBhCZnP"
   },
   "source": [
    "<h3>Распознавание спама.</h3>\n",
    "<p id=\"9\">Теперь рассмотрим задачу распознавания спама. Скачиваем датасет, где содержатся смс со спамом и не только.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHmG008TCZnP",
    "outputId": "363fd2d9-1ac4-4814-aa34-2d1fd17b4f8e"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFUCg0jXCZnQ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_table('SMSSpamCollection',sep='\\t',header=None, names=['label','sms_message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60UJktW7Fes2"
   },
   "source": [
    "Содержание датасета примитивно. Текст смс и спам/не спам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-kEUEuvVCZnR",
    "outputId": "9ec95f06-627c-4939-e70f-a55a4700b550"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pHYTFS8F9q8"
   },
   "source": [
    "Рассмотрим два подхода решения задачи. Сначала посмотрим на логистическую регрессию + TF IDF. Посчитаем коэффициент TF IDF (важность) для каждого класса (спам/не спам). Для этого объединим все спам смс в одно предложение, а не спам в другое.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCX5NvkQFpz9"
   },
   "outputs": [],
   "source": [
    "data_corp = [ \" \".join(df[df['label'] == l]['sms_message'].tolist()) for l in list(df.label.unique()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zxSnCxRFqnB",
    "outputId": "0cfa55d4-46b0-4476-8a50-93dea213e927"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() # обучаем Vectorizer на наших данных\n",
    "vectorizer.fit(data_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyJEIQ5bG-ME"
   },
   "source": [
    "Теперь получаем итоговые вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NphBJkNFq_R"
   },
   "outputs": [],
   "source": [
    "res_tfidf = vectorizer.transform(df['sms_message'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORc-VHkIH5lc"
   },
   "source": [
    "Теперь будем обучать логистическую регрессию. Имеем 4825 объектов спама, и 747 объектов не спама."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsLYmDxAG7dY",
    "outputId": "c673cf7c-f918-4afd-a118-7320ec24bb36"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder().fit(df['label'])\n",
    "df['cat_label'] = le.transform(df['label'])\n",
    "df['cat_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7OCis5yIK1K"
   },
   "source": [
    "Используем, как и всегда, train_test_split для разделения данных на обучающую и тестовую выборки. Указываем test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKRVUj6WHswh"
   },
   "outputs": [],
   "source": [
    "X_tr, X_ts, y_tr, y_ts=train_test_split(res_tfidf, df['cat_label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKINYW8XIcsn"
   },
   "source": [
    "Получаем  f1 score порядка 0.82-0.88, даже без прдобработки данных. Довольно приятный результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wgc1GKtvIY2x",
    "outputId": "0b9ac7d5-ad07-472f-8471-117a6e739406"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X_tr, y_tr)\n",
    "y_pred = lr.predict(X_ts)\n",
    "f1_score(y_ts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvhp-_mIJ0YR"
   },
   "source": [
    "В завершение урока, посмотрим на наивный Байесовский классификатор на том же самом датасете спама."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ioEQqHjIZOX"
   },
   "outputs": [],
   "source": [
    "df['label'] = df.label.map({'ham':0,'spam':1}) # делаем метки класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL5Z_aRZKEDG"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zrC3kHIKJlF"
   },
   "source": [
    "Считаем количество слов в предложениях, получаем матрицу размером - количество предложений, умноженное на размер словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WG4V6ihvKIw6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "train_tensor = count_vector.fit_transform(X_train).toarray().astype('float')\n",
    "test_tensor = count_vector.transform(X_test).toarray().astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMTyqJ7rO1UO"
   },
   "source": [
    "Получаем тензоры с предложениями класса спам и не спам и соответствующие условные вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rffxwjG0KqrG"
   },
   "outputs": [],
   "source": [
    "spam_train_tensor = train_tensor[(y_train == 1).values]\n",
    "not_spam_train_tensor = train_tensor[(y_train == 0).values]\n",
    "\n",
    "p_w_spam = (spam_train_tensor.sum(axis=0)) / (spam_train_tensor.sum())\n",
    "p_w_not_spam = (not_spam_train_tensor.sum(axis=0)) / (not_spam_train_tensor.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Co80hnnPQ6O"
   },
   "source": [
    "Теперь посчитаем вероятности, что любое сообщение спам/не спам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AE5wQJ5CKtZM"
   },
   "outputs": [],
   "source": [
    "p_spam = (y_train == 1).values.sum() / len(y_train)\n",
    "p_not_spam = (y_train == 0).values.sum() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-kDT6nEK3JQ"
   },
   "outputs": [],
   "source": [
    "#проверим на одном семпле\n",
    "test_sample = test_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPYiBDtxNWbs"
   },
   "source": [
    "Получаем вероятности того, что данное сообщение спам / не спам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7Q6q5VzK5TN",
    "outputId": "911cd870-2ff1-46d0-bd4a-0d96a6fb5f77"
   },
   "outputs": [],
   "source": [
    "# за спам\n",
    "np.log(p_spam+2.71828182846)-1 + (np.log((test_sample*p_w_spam)+2.71828182846)-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "waEDCSi-LALg",
    "outputId": "475b3f06-bbd3-4d0a-a930-d2a9da8b5767"
   },
   "outputs": [],
   "source": [
    "#против спама\n",
    "np.log(p_not_spam+2.71828182846)-1 + (np.log((test_sample*p_w_not_spam)+2.71828182846)-1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krTuQGKPOE9W"
   },
   "source": [
    "Теперь посчитаем все предсказания как сравнение величинв за спам и против спама."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxPT-BXpOJ48"
   },
   "outputs": [],
   "source": [
    "y_pred = (np.log(p_spam) + (np.log((test_tensor*p_w_spam)+0.00000001)).sum(axis=1)) >= \\\n",
    " np.log(p_not_spam) + (np.log((test_tensor*p_w_not_spam)+0.00000001)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-Mh_n1DOYYG"
   },
   "source": [
    "Посчитаем точность модели как отношение количество совпадений к размеру выборки и сравним реальные метки с предсказанными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCba0hloORxo",
    "outputId": "411c97e4-1f5c-46b9-c0f1-c7d981b75634"
   },
   "outputs": [],
   "source": [
    "test = (y_pred.astype(int) == y_test.to_numpy())\n",
    "test.sum().item()/test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa0c4YLJOd4p"
   },
   "source": [
    "Получили результат намного лучше, чем в случае с  линейной регрессией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hlu5KiGOiqU"
   },
   "source": [
    "<h2>Summary</h2>\n",
    "<ol><li>Проблема <b>эффективного кодирования</b> текстовых данных всегда стояла очень остро и с каждым годом появляются все более эффективные методы, дающие возможность улучшить обучение моделей на <b>классификацию текстов</b>, <b>фильтрацию спама</b> и т.д.</li><li><b>Наивный байесовский</b> классификатор -  вероятностный классификатор, основанный на применении <b>теоремы Байеса</b> со строгими (наивными) предположениями о <b>независимости</b> объектов от других.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFHTsgqTYAy5"
   },
   "source": [
    "<h3>Вопросы для самопроверки</h3>\n",
    "<p><ol><li>В чем отличие традиционных методов работы с текстом (кодирования) от векторного представления.</li><li>Что \"наивного\" в байесовском классификаторе? И как это влияет на результат?</li></ol></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_TQB5PaYAy6"
   },
   "source": [
    "<h1>Спасибо за внимание!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkolXXBCYAy6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "work_with_textdata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
